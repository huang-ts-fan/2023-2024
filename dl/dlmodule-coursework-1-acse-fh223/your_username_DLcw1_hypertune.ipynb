{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9YehS8enAmDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Name***: [*FAN HUANG*]\n",
        "### ***username***: [*acse-fh223*]\n",
        "### ***CID***: [*01883792*]\n",
        "\n",
        "\n",
        "You can save this notebook in Colab by clicking `File` from the top menu, and then selecting `Download --> Download .ipynb`\n",
        "\n",
        "Make sure that when you save your notebook you have all the cells executed and you can see the outputs (livelossplot graphs, etc)"
      ],
      "metadata": {
        "id": "gD4VUfPGx8Oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tunning notebook\n",
        "\n",
        "Explain the steps and tests you do.\n",
        "\n",
        "Organise it well to show how the data you present here has helped you design your final network hyperparameters (that you will use for the final training in the `yourusername_DLcw1_clean.ipynb` notebook)."
      ],
      "metadata": {
        "id": "KyZTroXCzEop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main hyperparameters available for tuning in our constructed model are:\n",
        "\n",
        "*   batch_size\n",
        "*   epochs\n",
        "*   number of neurons\n",
        "*   dims_latent\n",
        "*   learning rate\n",
        "\n",
        "\n",
        "After adding filters to the image preprocessing process, we can also adjust:\n",
        "\n",
        "*   kernel and sigma sizes of Gaussian filters\n",
        "\n",
        "To measure the impact on the model caused by our modification of the hyperparameters, we judge mainly by looking at the values of the loss function, and by observing with the naked eye the clarity of the output image and the retained eigenvalues."
      ],
      "metadata": {
        "id": "2Z09WOTKPCUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**batch_size**:\n",
        "The batch size greatly affects the speed of our model training, a smaller batch may yield lower loss values but it will also slow down our model training, considering the quality and quantity of our training set, we finally chose to set the batch_size to 200."
      ],
      "metadata": {
        "id": "hpDG__RY2NUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**epochs**:\n",
        "The number of iterations is a major factor that affects the training time of our model; typically, a higher number of iterations will allow the model to learn more features for better performance, but it will also keep increasing the time we have to wait for the model to train.\n",
        "\n",
        "Keeping the other hyperparameters consistent, we can observe the loss curve and find that although our loss curve has been decreasing as the number of epochs increases, the rate of loss decrease is becoming slower and slower after the number of epochs reaches 20.\n",
        "\n",
        "So, when we modify the other hyperparameters, the epochs we set were fixed to 20 to reduce the time needed for model training, but in the final version submitted, we still took 50 epochs (a slightly more value) in the expectation that our model could achieve better results."
      ],
      "metadata": {
        "id": "U8C1O1Dr28-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**number of neurons**:\n",
        "Number of neurons can have a big impact in some neural networks, more neurons or more layers of the network can learn more features to get a better model.\n",
        "\n",
        "However, our attempts to adjust the number of neurons and increase the depth of the network did not yield excellent results to the naked eye on our dataset, and in order to minimise the waiting time for training, the final structure of the network presented is a simpler version.\n",
        "\n",
        "We also tried training with CNNs, unfortunately due to the very limited clarity of the dataset itself, the feature loss after convolution was so severe that the CNN model we trained could not even generate the approximate structure."
      ],
      "metadata": {
        "id": "F972UwNq49p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dims_latent**:\n",
        "The dimensionality of the latent space has a big impact on the image we end up generating, the higher the dimensionality, the more graphical features can be learnt, but there are also some features that we don't want (apertures in the picture, planes for placing the hands) and they are not always imaged better with increasing dimensionality. After we tried dims = [2, 3, 4, 8, 16], we found that our loss function degraded faster and reached a lower minimum in the dim = 3 model."
      ],
      "metadata": {
        "id": "0PB8JdaM6eSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**learning rate**:The default learning rate for the Adam optimiser is 0.001 (1e-3), we additionally tried lr = 1e-2 and 1e-4, higher learning rates made the loss function show greater fluctuations, lower learning rates made our model train more slowly, and in the end we took the default learning rate.\n"
      ],
      "metadata": {
        "id": "b6IOyqdVB-7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian filters**:Adding a Gaussian filter during preprocessing is to cope with the noise in the dataset, but since the dataset itself is a very, very small image, we need to be very careful about setting the filter; too large a kernel or too high a sigma value can cause the image to become even blurrier. After several attempts we chose a kernel size of 3*3 to make our images smoother while preserving as many features as possible.\n"
      ],
      "metadata": {
        "id": "4ic97c5e72Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also tried more preprocessing operations, but did not achieve better results and they were discarded in the final version:\n",
        "\n",
        "*   Apply interpolation to the image to enlarge the size.\n",
        "*   Set a custom threshold for grey scale conversion to try to manually remove noise from the image.\n",
        "\n",
        "Interpolation will make our image size bigger and clearer after training, but since we need to rescale the output image, more features will be lost instead.\n",
        "\n",
        "Whereas simply using a custom threshold for greyscale conversion doesn't do a good job of meeting expectations."
      ],
      "metadata": {
        "id": "ufpKsn3q9aqN"
      }
    }
  ]
}