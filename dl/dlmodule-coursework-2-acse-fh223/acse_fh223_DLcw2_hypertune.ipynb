{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9YehS8enAmDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Name***: [*FAN HUANG*]\n",
        "### ***username***: [*acse-fh223*]\n",
        "### ***CID***: [*01883792*]\n",
        "\n"
      ],
      "metadata": {
        "id": "gD4VUfPGx8Oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tunning notebook"
      ],
      "metadata": {
        "id": "KyZTroXCzEop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main hyperparameters available for tuning in our constructed model are:\n",
        "\n",
        "*   batch_size\n",
        "*   epochs\n",
        "*   learning rate\n",
        "\n",
        "\n",
        "To measure the impact on the model caused by our modification of the hyperparameters, we judge mainly by looking at the values of the accuracy on validation datasets."
      ],
      "metadata": {
        "id": "2Z09WOTKPCUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**batch_size**:\n",
        "Batch size affects the speed of model training to a great extent, in this task our training set is not very large and using a smaller batch size helps us to capture the details of the images better which is more helpful for our classification task. We tried batch sizes of 4, 8, 16, 32, 64, and 128, the performance of the model shakes very much when the batch size is greater than 64, We compared the speed and accuracy of model fitting and finally chose 16."
      ],
      "metadata": {
        "id": "hpDG__RY2NUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**epochs**:\n",
        "Typically, the more iterations, the more features the model learns and the better the performance, but in the case of our very small dataset, too many iterations can very easily cause overfitting. We set the epoch to 80 to ensure that the model does not overfit easily.\n"
      ],
      "metadata": {
        "id": "U8C1O1Dr28-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**learning rate**:This time the model accuracy oscillates over a range, with higher learning rates (0.002, 0.005, 0.01) showing greater fluctuations in the loss function, and lower learning rates making the model slower to train (0.0001) and not offering greater gains in performance, we ended up with lr = 1e-3.\n"
      ],
      "metadata": {
        "id": "b6IOyqdVB-7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also tried more preprocessing operations, but did not achieve better results and they were discarded in the final version:\n",
        "\n",
        "*   Crop images as new data\n",
        "*   Flip images horizontally\n",
        "*   Implement Gaussian filter on the original images.\n",
        "\n",
        "\n",
        "The cropping operation did not significantly improve the accuracy of the model, and image flipping even caused some dip in the model's performance.\n",
        "\n",
        "Gaussian filtering is usually an effective preprocessing method, but it is possible that due to the small size of our image, the filtered image loses some of the features used for classification, making the model training results worse."
      ],
      "metadata": {
        "id": "ufpKsn3q9aqN"
      }
    }
  ]
}